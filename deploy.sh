#!/bin/bash
set -e

# ============================================
# JapanPropSearch Scraper — Vultr VPS Deploy
# ============================================
# Deploys the Python scraper microservice with Supabase as the database.
# No local PostgreSQL — all data goes directly to Supabase.
#
# Usage:
#   ssh into VPS, clone repo, run: bash deploy.sh
#   OR: curl -fsSL https://raw.githubusercontent.com/<your-repo>/main/deploy.sh | bash

echo "=========================================="
echo "  JapanPropSearch Scraper — Deploy"
echo "=========================================="

# --- 1. Install Docker ---
if ! command -v docker &> /dev/null; then
    echo "[1/5] Installing Docker..."
    apt-get update -qq
    apt-get install -y -qq ca-certificates curl gnupg
    install -m 0755 -d /etc/apt/keyrings
    curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /etc/apt/keyrings/docker.gpg
    chmod a+r /etc/apt/keyrings/docker.gpg
    echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | tee /etc/apt/sources.list.d/docker.list > /dev/null
    apt-get update -qq
    apt-get install -y -qq docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
    echo "[1/5] Docker installed."
else
    echo "[1/5] Docker already installed, skipping."
fi

# --- 2. Setup project directory ---
APP_DIR="/opt/japan-reit"
echo "[2/5] Setting up project directory at $APP_DIR..."

if [ ! -d "$APP_DIR" ]; then
    if [ -f "docker-compose.prod.yml" ]; then
        cp -r "$(pwd)" "$APP_DIR"
    else
        echo "ERROR: Run this script from the project root (where docker-compose.prod.yml is)"
        echo "OR clone the repo first: git clone <your-repo> $APP_DIR"
        exit 1
    fi
fi
cd "$APP_DIR"

# --- 3. Configure production environment ---
ENV_FILE="$APP_DIR/.env.production"
if [ ! -f "$ENV_FILE" ] || grep -q "CHANGE_ME" "$ENV_FILE"; then
    echo "[3/5] Configuring production environment..."
    echo ""
    echo "  You need the following from your Supabase dashboard:"
    echo "  - Database connection string (Settings > Database > Connection string > URI)"
    echo "  - Project URL (Settings > API > Project URL)"
    echo "  - Service Role Key (Settings > API > service_role key)"
    echo ""

    # DATABASE_URL
    read -p "  Supabase DATABASE_URL (postgresql://...): " DB_URL
    if [ -z "$DB_URL" ]; then
        echo "ERROR: DATABASE_URL is required"
        exit 1
    fi

    # SCRAPER_API_KEY
    API_KEY=$(openssl rand -base64 32 | tr -d '/+=' | head -c 40)
    echo "  Generated SCRAPER_API_KEY: $API_KEY"
    echo "  (Save this — the Next.js app needs it as SCRAPER_API_KEY env var)"

    # SUPABASE_URL
    read -p "  Supabase Project URL (https://xxx.supabase.co): " SUPA_URL

    # SUPABASE_SERVICE_ROLE_KEY
    read -p "  Supabase Service Role Key: " SUPA_KEY

    # GOOGLE_TRANSLATE_API_KEY
    read -p "  Google Translate API Key (or press Enter to skip): " TRANSLATE_KEY

    # CORS origins
    read -p "  Vercel app URL (e.g. https://japan-prop-search.vercel.app): " VERCEL_URL
    CORS="https://japan-prop-search.vercel.app,http://localhost:3000"
    if [ -n "$VERCEL_URL" ]; then
        CORS="$VERCEL_URL,http://localhost:3000"
    fi

    cat > "$ENV_FILE" <<EOF
# Auto-generated by deploy.sh on $(date -Iseconds)
# JapanPropSearch Scraper — Production Environment

# Database — Supabase PostgreSQL
DATABASE_URL=$DB_URL

# API Security
SCRAPER_API_KEY=$API_KEY

# Supabase Storage (image uploads)
SUPABASE_URL=$SUPA_URL
SUPABASE_SERVICE_ROLE_KEY=$SUPA_KEY
SUPABASE_STORAGE_BUCKET=property-images

# Google Translate (optional — scraper works without it)
GOOGLE_TRANSLATE_API_KEY=$TRANSLATE_KEY

# Hazard data (optional)
REINFOLIB_API_KEY=

# Scheduler — enable after first successful manual scrape
SCHEDULER_ENABLED=false
SCHEDULER_INTERVAL_HOURS=6

# CORS — allowed origins
CORS_ORIGINS=$CORS

# App
APP_PORT=80
EOF

    echo ""
    echo "  Configuration saved to $ENV_FILE"
    echo ""
else
    echo "[3/5] .env.production already configured, skipping."
    echo "  To reconfigure, delete $ENV_FILE and re-run deploy.sh"
fi

# --- 4. Build & Start ---
echo "[4/5] Building and starting services (this takes 3-5 minutes on first run)..."
docker compose -f docker-compose.prod.yml --env-file .env.production up -d --build

# --- 5. Verify ---
echo "[5/5] Verifying deployment..."
echo "  Waiting for services to start (Playwright install takes ~30s)..."
sleep 15

VPS_IP=$(curl -sf ifconfig.me 2>/dev/null || echo '<your-vps-ip>')

# Check health
if curl -sf http://localhost/health > /dev/null 2>&1; then
    HEALTH=$(curl -sf http://localhost/health)
    echo ""
    echo "=========================================="
    echo "  DEPLOYMENT SUCCESSFUL"
    echo "=========================================="
    echo ""
    echo "  Health:  $HEALTH" | head -c 200
    echo ""
    echo ""
    echo "  App:     http://$VPS_IP"
    echo "  API:     http://$VPS_IP/docs"
    echo "  Health:  http://$VPS_IP/health"
    echo "  Config:  $ENV_FILE"
    echo ""
    echo "  Next steps:"
    echo "    1. Test: curl -X POST http://$VPS_IP/api/scrape/trigger \\"
    echo "         -H 'X-API-Key: $API_KEY' \\"
    echo "         -H 'Content-Type: application/json' \\"
    echo "         -d '{\"source_id\": \"suumo\", \"prefecture_code\": \"13\"}'"
    echo "    2. If scrape succeeds, enable scheduler:"
    echo "       Edit $ENV_FILE → SCHEDULER_ENABLED=true"
    echo "       docker compose -f docker-compose.prod.yml --env-file .env.production restart backend"
    echo ""
    echo "  Useful commands:"
    echo "    cd $APP_DIR"
    echo "    docker compose -f docker-compose.prod.yml --env-file .env.production logs -f backend"
    echo "    docker compose -f docker-compose.prod.yml --env-file .env.production restart"
    echo "    docker compose -f docker-compose.prod.yml --env-file .env.production down"
    echo ""
else
    echo ""
    echo "  Backend is still starting... check in 30-60 seconds:"
    echo "    curl http://localhost/health"
    echo "    docker compose -f docker-compose.prod.yml --env-file .env.production logs -f backend"
    echo ""
fi
